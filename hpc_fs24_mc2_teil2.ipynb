{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"numba\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 48 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda': {'time_transfer_ms': 0.9606400094926357,\n",
       "  'time_kernel_ms': 0.07952000200748444,\n",
       "  'time_reco_ms': 1.0401600115001202,\n",
       "  'consumed_mem_bandwidth_GB/s': 1490.8007671936703,\n",
       "  'consumed GFLOPs': 279.11468108251535},\n",
       " 'cpu': {'time_reco_ms': 0.0,\n",
       "  'consumed_mem_bandwidth_GB/s': inf,\n",
       "  'consumed GFLOPs': inf},\n",
       " 'cpu_cuda_same': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cuda_kernel import svd_reco_cuda_perfmeasure\n",
    "import os\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "subfolder = '001'\n",
    "folders = os.path.join('adni_png', subfolder)\n",
    "\n",
    "# Get all PNGs from 001 with 145 in the name\n",
    "files = sorted(glob.glob(f\"{folders}/*145.png\"))\n",
    "\n",
    "# Load all images using ImageIO and create a numpy array from them\n",
    "images = np.array([imageio.imread(f) for f in files])\n",
    "\n",
    "# Get all the names of the files\n",
    "names = [f[-17:-4] for f in files]\n",
    "\n",
    "im = images[0]\n",
    "im = im - im.min() / im.max() - im.min()  # normalize image\n",
    "u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "svd_reco_cuda_perfmeasure(u, s, vt, len(s), (32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:02<00:00, 12.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "matrix_sizes = [int(size) for size in np.linspace(20, 5000, 10).astype(int)]\n",
    "block_sizes_warpfriendly = [8, 16, 32]\n",
    "\n",
    "results = []\n",
    "for matrix_size in tqdm(matrix_sizes):\n",
    "    # random image\n",
    "    im = np.random.normal(size=(matrix_size, matrix_size))\n",
    "\n",
    "    # normalize image\n",
    "    im = im - im.min() / im.max() - im.min()  \n",
    "\n",
    "    # decomposition\n",
    "    u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "    # reconstruct with different block sizes\n",
    "    for block_size in block_sizes_warpfriendly:\n",
    "        result = pd.json_normalize(svd_reco_cuda_perfmeasure(u, s, vt, len(s), (block_size, block_size)))\n",
    "        result[[\"matrix_size\", \"block_size\"]] = [[matrix_size, block_size]]\n",
    "        results.append(result)\n",
    "\n",
    "        # wait 2 seconds for gpu to do cleanup\n",
    "        time.sleep(2)\n",
    "        \n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_cuda_same</th>\n",
       "      <th>cuda.time_transfer_ms</th>\n",
       "      <th>cuda.time_kernel_ms</th>\n",
       "      <th>cuda.time_reco_ms</th>\n",
       "      <th>cuda.consumed_mem_bandwidth_GB/s</th>\n",
       "      <th>cuda.consumed GFLOPs</th>\n",
       "      <th>cpu.time_reco_ms</th>\n",
       "      <th>cpu.consumed_mem_bandwidth_GB/s</th>\n",
       "      <th>cpu.consumed GFLOPs</th>\n",
       "      <th>matrix_size</th>\n",
       "      <th>block_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>3.240960</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>3.274752</td>\n",
       "      <td>3.835227</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.412672</td>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.456704</td>\n",
       "      <td>2.943314</td>\n",
       "      <td>0.545058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1.030144</td>\n",
       "      <td>0.087040</td>\n",
       "      <td>1.117184</td>\n",
       "      <td>1.488971</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.384640</td>\n",
       "      <td>26.429440</td>\n",
       "      <td>32.814079</td>\n",
       "      <td>113.942393</td>\n",
       "      <td>21.354882</td>\n",
       "      <td>2.000332</td>\n",
       "      <td>1505.466978</td>\n",
       "      <td>282.151955</td>\n",
       "      <td>573</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.275072</td>\n",
       "      <td>8.482816</td>\n",
       "      <td>14.757888</td>\n",
       "      <td>355.004008</td>\n",
       "      <td>66.534223</td>\n",
       "      <td>3.004313</td>\n",
       "      <td>1002.370284</td>\n",
       "      <td>187.862464</td>\n",
       "      <td>573</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>8.154112</td>\n",
       "      <td>9.350144</td>\n",
       "      <td>17.504256</td>\n",
       "      <td>322.073485</td>\n",
       "      <td>60.362442</td>\n",
       "      <td>2.790928</td>\n",
       "      <td>1079.008025</td>\n",
       "      <td>202.225774</td>\n",
       "      <td>573</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>23.856128</td>\n",
       "      <td>144.944122</td>\n",
       "      <td>168.800251</td>\n",
       "      <td>157.627127</td>\n",
       "      <td>29.548526</td>\n",
       "      <td>12.000084</td>\n",
       "      <td>1903.913811</td>\n",
       "      <td>356.904598</td>\n",
       "      <td>1126</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>31.732737</td>\n",
       "      <td>136.718338</td>\n",
       "      <td>168.451075</td>\n",
       "      <td>167.110907</td>\n",
       "      <td>31.326340</td>\n",
       "      <td>14.999151</td>\n",
       "      <td>1523.227893</td>\n",
       "      <td>285.541832</td>\n",
       "      <td>1126</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>29.697024</td>\n",
       "      <td>82.272255</td>\n",
       "      <td>111.969279</td>\n",
       "      <td>277.701462</td>\n",
       "      <td>52.057466</td>\n",
       "      <td>15.113354</td>\n",
       "      <td>1511.717778</td>\n",
       "      <td>283.384165</td>\n",
       "      <td>1126</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>46.669409</td>\n",
       "      <td>301.871460</td>\n",
       "      <td>348.540869</td>\n",
       "      <td>251.356659</td>\n",
       "      <td>47.122361</td>\n",
       "      <td>28.003216</td>\n",
       "      <td>2709.596004</td>\n",
       "      <td>507.973659</td>\n",
       "      <td>1680</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>13.376512</td>\n",
       "      <td>81.341438</td>\n",
       "      <td>94.717950</td>\n",
       "      <td>932.825915</td>\n",
       "      <td>174.878835</td>\n",
       "      <td>34.003258</td>\n",
       "      <td>2231.474471</td>\n",
       "      <td>418.339210</td>\n",
       "      <td>1680</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>44.837887</td>\n",
       "      <td>277.789703</td>\n",
       "      <td>322.627590</td>\n",
       "      <td>273.146919</td>\n",
       "      <td>51.207427</td>\n",
       "      <td>37.071705</td>\n",
       "      <td>2046.773999</td>\n",
       "      <td>383.713025</td>\n",
       "      <td>1680</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>75.982815</td>\n",
       "      <td>256.060425</td>\n",
       "      <td>332.043240</td>\n",
       "      <td>695.812634</td>\n",
       "      <td>130.450264</td>\n",
       "      <td>55.000782</td>\n",
       "      <td>3239.409914</td>\n",
       "      <td>607.321365</td>\n",
       "      <td>2233</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>41.497727</td>\n",
       "      <td>170.224640</td>\n",
       "      <td>211.722367</td>\n",
       "      <td>1046.676196</td>\n",
       "      <td>196.229817</td>\n",
       "      <td>67.000866</td>\n",
       "      <td>2659.220535</td>\n",
       "      <td>498.548034</td>\n",
       "      <td>2233</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>41.564161</td>\n",
       "      <td>189.296646</td>\n",
       "      <td>230.860807</td>\n",
       "      <td>941.221528</td>\n",
       "      <td>176.459281</td>\n",
       "      <td>78.058481</td>\n",
       "      <td>2282.520436</td>\n",
       "      <td>427.924673</td>\n",
       "      <td>2233</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>113.364291</td>\n",
       "      <td>294.205444</td>\n",
       "      <td>407.569736</td>\n",
       "      <td>1176.119859</td>\n",
       "      <td>220.502687</td>\n",
       "      <td>100.000858</td>\n",
       "      <td>3460.178958</td>\n",
       "      <td>648.725342</td>\n",
       "      <td>2786</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>63.092831</td>\n",
       "      <td>264.983551</td>\n",
       "      <td>328.076382</td>\n",
       "      <td>1305.820170</td>\n",
       "      <td>244.819313</td>\n",
       "      <td>93.998432</td>\n",
       "      <td>3681.134437</td>\n",
       "      <td>690.150777</td>\n",
       "      <td>2786</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>66.741663</td>\n",
       "      <td>270.526428</td>\n",
       "      <td>337.268091</td>\n",
       "      <td>1279.064925</td>\n",
       "      <td>239.803155</td>\n",
       "      <td>106.061935</td>\n",
       "      <td>3262.441556</td>\n",
       "      <td>611.652906</td>\n",
       "      <td>2786</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>165.761030</td>\n",
       "      <td>343.695374</td>\n",
       "      <td>509.456404</td>\n",
       "      <td>1734.675333</td>\n",
       "      <td>325.227282</td>\n",
       "      <td>159.000635</td>\n",
       "      <td>3749.669842</td>\n",
       "      <td>703.010475</td>\n",
       "      <td>3340</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>48.033441</td>\n",
       "      <td>333.644714</td>\n",
       "      <td>381.678155</td>\n",
       "      <td>1786.930411</td>\n",
       "      <td>335.024375</td>\n",
       "      <td>163.051367</td>\n",
       "      <td>3656.515723</td>\n",
       "      <td>685.545385</td>\n",
       "      <td>3340</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>93.398076</td>\n",
       "      <td>324.933624</td>\n",
       "      <td>418.331700</td>\n",
       "      <td>1834.835923</td>\n",
       "      <td>344.005987</td>\n",
       "      <td>161.034107</td>\n",
       "      <td>3702.320563</td>\n",
       "      <td>694.133149</td>\n",
       "      <td>3340</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>218.786816</td>\n",
       "      <td>314.969086</td>\n",
       "      <td>533.755901</td>\n",
       "      <td>2997.320283</td>\n",
       "      <td>561.961465</td>\n",
       "      <td>236.001015</td>\n",
       "      <td>4000.250720</td>\n",
       "      <td>749.998847</td>\n",
       "      <td>3893</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>67.294209</td>\n",
       "      <td>381.350922</td>\n",
       "      <td>448.645131</td>\n",
       "      <td>2475.576105</td>\n",
       "      <td>464.140714</td>\n",
       "      <td>228.868961</td>\n",
       "      <td>4124.907212</td>\n",
       "      <td>773.370438</td>\n",
       "      <td>3893</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>68.940738</td>\n",
       "      <td>421.704712</td>\n",
       "      <td>490.645450</td>\n",
       "      <td>2238.683141</td>\n",
       "      <td>419.726135</td>\n",
       "      <td>229.000092</td>\n",
       "      <td>4122.545204</td>\n",
       "      <td>772.927590</td>\n",
       "      <td>3893</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>222.783774</td>\n",
       "      <td>371.843048</td>\n",
       "      <td>594.626822</td>\n",
       "      <td>3781.752574</td>\n",
       "      <td>709.038738</td>\n",
       "      <td>335.010290</td>\n",
       "      <td>4197.537943</td>\n",
       "      <td>786.994111</td>\n",
       "      <td>4446</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>86.672478</td>\n",
       "      <td>482.600922</td>\n",
       "      <td>569.273399</td>\n",
       "      <td>2913.832820</td>\n",
       "      <td>546.312934</td>\n",
       "      <td>325.006485</td>\n",
       "      <td>4326.739524</td>\n",
       "      <td>811.218046</td>\n",
       "      <td>4446</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>89.660516</td>\n",
       "      <td>423.061493</td>\n",
       "      <td>512.722009</td>\n",
       "      <td>3323.910183</td>\n",
       "      <td>623.198117</td>\n",
       "      <td>323.000193</td>\n",
       "      <td>4353.614754</td>\n",
       "      <td>816.256868</td>\n",
       "      <td>4446</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>293.948420</td>\n",
       "      <td>536.378418</td>\n",
       "      <td>830.326838</td>\n",
       "      <td>3728.897236</td>\n",
       "      <td>699.133275</td>\n",
       "      <td>457.998753</td>\n",
       "      <td>4367.042462</td>\n",
       "      <td>818.779523</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>110.252191</td>\n",
       "      <td>612.632568</td>\n",
       "      <td>722.884759</td>\n",
       "      <td>3264.762769</td>\n",
       "      <td>612.112413</td>\n",
       "      <td>451.999903</td>\n",
       "      <td>4425.000952</td>\n",
       "      <td>829.646196</td>\n",
       "      <td>5000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>106.699487</td>\n",
       "      <td>584.927429</td>\n",
       "      <td>691.626916</td>\n",
       "      <td>3419.398544</td>\n",
       "      <td>641.105172</td>\n",
       "      <td>446.999550</td>\n",
       "      <td>4474.501150</td>\n",
       "      <td>838.927019</td>\n",
       "      <td>5000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cpu_cuda_same  cuda.time_transfer_ms  cuda.time_kernel_ms  \\\n",
       "0           True               3.240960             0.033792   \n",
       "0           True               0.412672             0.044032   \n",
       "0           True               1.030144             0.087040   \n",
       "0           True               6.384640            26.429440   \n",
       "0           True               6.275072             8.482816   \n",
       "0           True               8.154112             9.350144   \n",
       "0           True              23.856128           144.944122   \n",
       "0           True              31.732737           136.718338   \n",
       "0           True              29.697024            82.272255   \n",
       "0           True              46.669409           301.871460   \n",
       "0           True              13.376512            81.341438   \n",
       "0           True              44.837887           277.789703   \n",
       "0           True              75.982815           256.060425   \n",
       "0           True              41.497727           170.224640   \n",
       "0           True              41.564161           189.296646   \n",
       "0           True             113.364291           294.205444   \n",
       "0           True              63.092831           264.983551   \n",
       "0           True              66.741663           270.526428   \n",
       "0           True             165.761030           343.695374   \n",
       "0           True              48.033441           333.644714   \n",
       "0           True              93.398076           324.933624   \n",
       "0           True             218.786816           314.969086   \n",
       "0           True              67.294209           381.350922   \n",
       "0           True              68.940738           421.704712   \n",
       "0           True             222.783774           371.843048   \n",
       "0           True              86.672478           482.600922   \n",
       "0           True              89.660516           423.061493   \n",
       "0           True             293.948420           536.378418   \n",
       "0           True             110.252191           612.632568   \n",
       "0           True             106.699487           584.927429   \n",
       "\n",
       "   cuda.time_reco_ms  cuda.consumed_mem_bandwidth_GB/s  cuda.consumed GFLOPs  \\\n",
       "0           3.274752                          3.835227              0.710227   \n",
       "0           0.456704                          2.943314              0.545058   \n",
       "0           1.117184                          1.488971              0.275735   \n",
       "0          32.814079                        113.942393             21.354882   \n",
       "0          14.757888                        355.004008             66.534223   \n",
       "0          17.504256                        322.073485             60.362442   \n",
       "0         168.800251                        157.627127             29.548526   \n",
       "0         168.451075                        167.110907             31.326340   \n",
       "0         111.969279                        277.701462             52.057466   \n",
       "0         348.540869                        251.356659             47.122361   \n",
       "0          94.717950                        932.825915            174.878835   \n",
       "0         322.627590                        273.146919             51.207427   \n",
       "0         332.043240                        695.812634            130.450264   \n",
       "0         211.722367                       1046.676196            196.229817   \n",
       "0         230.860807                        941.221528            176.459281   \n",
       "0         407.569736                       1176.119859            220.502687   \n",
       "0         328.076382                       1305.820170            244.819313   \n",
       "0         337.268091                       1279.064925            239.803155   \n",
       "0         509.456404                       1734.675333            325.227282   \n",
       "0         381.678155                       1786.930411            335.024375   \n",
       "0         418.331700                       1834.835923            344.005987   \n",
       "0         533.755901                       2997.320283            561.961465   \n",
       "0         448.645131                       2475.576105            464.140714   \n",
       "0         490.645450                       2238.683141            419.726135   \n",
       "0         594.626822                       3781.752574            709.038738   \n",
       "0         569.273399                       2913.832820            546.312934   \n",
       "0         512.722009                       3323.910183            623.198117   \n",
       "0         830.326838                       3728.897236            699.133275   \n",
       "0         722.884759                       3264.762769            612.112413   \n",
       "0         691.626916                       3419.398544            641.105172   \n",
       "\n",
       "   cpu.time_reco_ms  cpu.consumed_mem_bandwidth_GB/s  cpu.consumed GFLOPs  \\\n",
       "0          0.000000                              inf                  inf   \n",
       "0          0.000000                              inf                  inf   \n",
       "0          0.000000                              inf                  inf   \n",
       "0          2.000332                      1505.466978           282.151955   \n",
       "0          3.004313                      1002.370284           187.862464   \n",
       "0          2.790928                      1079.008025           202.225774   \n",
       "0         12.000084                      1903.913811           356.904598   \n",
       "0         14.999151                      1523.227893           285.541832   \n",
       "0         15.113354                      1511.717778           283.384165   \n",
       "0         28.003216                      2709.596004           507.973659   \n",
       "0         34.003258                      2231.474471           418.339210   \n",
       "0         37.071705                      2046.773999           383.713025   \n",
       "0         55.000782                      3239.409914           607.321365   \n",
       "0         67.000866                      2659.220535           498.548034   \n",
       "0         78.058481                      2282.520436           427.924673   \n",
       "0        100.000858                      3460.178958           648.725342   \n",
       "0         93.998432                      3681.134437           690.150777   \n",
       "0        106.061935                      3262.441556           611.652906   \n",
       "0        159.000635                      3749.669842           703.010475   \n",
       "0        163.051367                      3656.515723           685.545385   \n",
       "0        161.034107                      3702.320563           694.133149   \n",
       "0        236.001015                      4000.250720           749.998847   \n",
       "0        228.868961                      4124.907212           773.370438   \n",
       "0        229.000092                      4122.545204           772.927590   \n",
       "0        335.010290                      4197.537943           786.994111   \n",
       "0        325.006485                      4326.739524           811.218046   \n",
       "0        323.000193                      4353.614754           816.256868   \n",
       "0        457.998753                      4367.042462           818.779523   \n",
       "0        451.999903                      4425.000952           829.646196   \n",
       "0        446.999550                      4474.501150           838.927019   \n",
       "\n",
       "   matrix_size  block_size  \n",
       "0           20           8  \n",
       "0           20          16  \n",
       "0           20          32  \n",
       "0          573           8  \n",
       "0          573          16  \n",
       "0          573          32  \n",
       "0         1126           8  \n",
       "0         1126          16  \n",
       "0         1126          32  \n",
       "0         1680           8  \n",
       "0         1680          16  \n",
       "0         1680          32  \n",
       "0         2233           8  \n",
       "0         2233          16  \n",
       "0         2233          32  \n",
       "0         2786           8  \n",
       "0         2786          16  \n",
       "0         2786          32  \n",
       "0         3340           8  \n",
       "0         3340          16  \n",
       "0         3340          32  \n",
       "0         3893           8  \n",
       "0         3893          16  \n",
       "0         3893          32  \n",
       "0         4446           8  \n",
       "0         4446          16  \n",
       "0         4446          32  \n",
       "0         5000           8  \n",
       "0         5000          16  \n",
       "0         5000          32  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-d94spYeM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
