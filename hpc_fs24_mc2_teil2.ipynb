{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System info:\n",
      "/home/buesst1/.cache/pypoetry/virtualenvs/hpc-mc2-buesst1-bcMvFOTC-py3.11/lib/python3.11/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "__Time Stamp__\n",
      "Report started (local time)                   : 2024-11-18 20:32:45.172829\n",
      "UTC start time                                : 2024-11-18 19:32:45.172831\n",
      "Running time (s)                              : 0.566623\n",
      "\n",
      "__Hardware Information__\n",
      "Machine                                       : x86_64\n",
      "CPU Name                                      : goldmont\n",
      "CPU Count                                     : 32\n",
      "Number of accessible CPUs                     : 32\n",
      "List of accessible CPUs cores                 : 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      "CFS Restrictions (CPUs worth of runtime)      : None\n",
      "\n",
      "CPU Features                                  : 64bit adx aes avx avx2 bmi bmi2\n",
      "                                                clflushopt clwb cmov crc32 cx16\n",
      "                                                cx8 f16c fma fsgsbase fxsr gfni\n",
      "                                                invpcid lzcnt mmx movbe pclmul\n",
      "                                                popcnt prfchw rdpid rdrnd rdseed\n",
      "                                                sahf sha sse sse2 sse3 sse4.1\n",
      "                                                sse4.2 ssse3 vaes vpclmulqdq xsave\n",
      "                                                xsavec xsaveopt xsaves\n",
      "\n",
      "Memory Total (MB)                             : 96557\n",
      "Memory Available (MB)                         : 93991\n",
      "\n",
      "__OS Information__\n",
      "Platform Name                                 : Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Platform Release                              : 5.15.146.1-microsoft-standard-WSL2\n",
      "OS Name                                       : Linux\n",
      "OS Version                                    : #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "OS Specific Version                           : ?\n",
      "Libc Version                                  : glibc 2.35\n",
      "\n",
      "__Python Information__\n",
      "Python Compiler                               : GCC 11.4.0\n",
      "Python Implementation                         : CPython\n",
      "Python Version                                : 3.11.9\n",
      "Python Locale                                 : en_US.UTF-8\n",
      "\n",
      "__Numba Toolchain Versions__\n",
      "Numba Version                                 : 0.60.0\n",
      "llvmlite Version                              : 0.43.0\n",
      "\n",
      "__LLVM Information__\n",
      "LLVM Version                                  : 14.0.6\n",
      "\n",
      "__CUDA Information__\n",
      "CUDA Device Initialized                       : True\n",
      "CUDA Driver Version                           : 12.4\n",
      "CUDA Runtime Version                          : 11.5\n",
      "CUDA NVIDIA Bindings Available                : False\n",
      "CUDA NVIDIA Bindings In Use                   : False\n",
      "CUDA Minor Version Compatibility Available    : False\n",
      "CUDA Minor Version Compatibility Needed       : False\n",
      "CUDA Minor Version Compatibility In Use       : False\n",
      "CUDA Detect Output:\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 4090'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.9\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-7c34b908-8185-6853-7d22-ec916e7af144\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 64\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA Libraries Test Output:\n",
      "Finding driver from candidates:\n",
      "\t/usr/lib/wsl/lib/libcuda.so.1\n",
      "Using loader <class 'ctypes.CDLL'>\n",
      "\tTrying to load driver...\tok\n",
      "\t\tLoaded from /usr/lib/wsl/lib/libcuda.so.1\n",
      "\tMapped libcuda.so paths:\n",
      "\t\t/usr/lib/wsl/drivers/nvmdi.inf_amd64_5709e141414310f9/libcuda.so.1.1\n",
      "\t\t/usr/lib/wsl/lib/libcuda.so.1\n",
      "Finding nvvm from <unknown>\n",
      "\tLocated at libnvvm.so\n",
      "\tTrying to open library...\tok\n",
      "Finding nvrtc from <unknown>\n",
      "\tLocated at libnvrtc.so\n",
      "\tTrying to open library...\tok\n",
      "Finding cudart from <unknown>\n",
      "\tLocated at libcudart.so\n",
      "\tTrying to open library...\tok\n",
      "Finding cudadevrt from <unknown>\n",
      "\tLocated at libcudadevrt.a\n",
      "\tChecking library...\tERROR: failed to find cudadevrt:\n",
      "libcudadevrt.a not found\n",
      "Finding libdevice from Debian package\n",
      "\tLocated at /usr/lib/nvidia-cuda-toolkit/libdevice/libdevice.10.bc\n",
      "\tChecking library...\tok\n",
      "\n",
      "\n",
      "__NumPy Information__\n",
      "NumPy Version                                 : 2.0.2\n",
      "NumPy Supported SIMD features                 : ('MMX', 'SSE', 'SSE2', 'SSE3', 'SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2')\n",
      "NumPy Supported SIMD dispatch                 : ('SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2', 'AVX512F', 'AVX512CD', 'AVX512_KNL', 'AVX512_KNM', 'AVX512_SKX', 'AVX512_CLX', 'AVX512_CNL', 'AVX512_ICL')\n",
      "NumPy Supported SIMD baseline                 : ('SSE', 'SSE2', 'SSE3')\n",
      "NumPy AVX512_SKX support detected             : False\n",
      "\n",
      "__SVML Information__\n",
      "SVML State, config.USING_SVML                 : False\n",
      "SVML Library Loaded                           : False\n",
      "llvmlite Using SVML Patched LLVM              : True\n",
      "SVML Operational                              : False\n",
      "\n",
      "__Threading Layer Information__\n",
      "TBB Threading Layer Available                 : False\n",
      "+--> Disabled due to Unknown import problem.\n",
      "OpenMP Threading Layer Available              : True\n",
      "+-->Vendor: GNU\n",
      "Workqueue Threading Layer Available           : True\n",
      "+-->Workqueue imported successfully.\n",
      "\n",
      "__Numba Environment Variable Information__\n",
      "NUMBA_CUDA_DRIVER                             : /usr/lib/wsl/lib/libcuda.so.1\n",
      "\n",
      "__Conda Information__\n",
      "Conda not available.\n",
      "\n",
      "__Installed Packages__\n",
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         2.4.1\n",
      "comm              0.2.2\n",
      "contourpy         1.3.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.7\n",
      "decorator         5.1.1\n",
      "executing         2.1.0\n",
      "fonttools         4.54.1\n",
      "imageio           2.36.0\n",
      "ipykernel         6.29.5\n",
      "ipython           8.29.0\n",
      "jedi              0.19.1\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.7\n",
      "lazy_loader       0.4\n",
      "llvmlite          0.43.0\n",
      "matplotlib        3.9.2\n",
      "matplotlib-inline 0.1.7\n",
      "nest-asyncio      1.6.0\n",
      "networkx          3.4.2\n",
      "numba             0.60.0\n",
      "numpy             2.0.2\n",
      "packaging         24.1\n",
      "pandas            2.2.3\n",
      "parso             0.8.4\n",
      "pexpect           4.9.0\n",
      "pillow            11.0.0\n",
      "pip               24.0\n",
      "platformdirs      4.3.6\n",
      "prompt_toolkit    3.0.48\n",
      "psutil            6.1.0\n",
      "ptyprocess        0.7.0\n",
      "pure_eval         0.2.3\n",
      "Pygments          2.18.0\n",
      "pyparsing         3.2.0\n",
      "python-dateutil   2.9.0.post0\n",
      "pytz              2024.2\n",
      "pyzmq             26.2.0\n",
      "scikit-image      0.24.0\n",
      "scipy             1.14.1\n",
      "seaborn           0.13.2\n",
      "setuptools        69.5.1\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tifffile          2024.9.20\n",
      "tornado           6.4.1\n",
      "traitlets         5.14.3\n",
      "typing_extensions 4.12.2\n",
      "tzdata            2024.2\n",
      "wcwidth           0.2.13\n",
      "\n",
      "No errors reported.\n",
      "\n",
      "\n",
      "__Warning log__\n",
      "Warning: Conda not available.\n",
      " Error was [Errno 2] No such file or directory: 'conda'\n",
      "\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_quota_us\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_period_us\n",
      "--------------------------------------------------------------------------------\n",
      "If requested, please copy and paste the information between\n",
      "the dashed (----) lines, or from a given specific section as\n",
      "appropriate.\n",
      "\n",
      "=============================================================\n",
      "IMPORTANT: Please ensure that you are happy with sharing the\n",
      "contents of the information present, any information that you\n",
      "wish to keep private you should remove before sharing.\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buesst1/.cache/pypoetry/virtualenvs/hpc-mc2-buesst1-bcMvFOTC-py3.11/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 6 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_offset = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + (self._t_end - self._t_start)\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_offset = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + 1e-3*cuda.event_elapsed_time(self._t_start, self._t_end)\n",
    "\n",
    "@cuda.jit(\"void(Array(float32, 2, 'C'), Array(float32, 1, 'C'), Array(float32, 2, 'F'), int32, Array(float32, 2, 'C'))\")\n",
    "def svd_reco_kernel(u, s, vt, k, y):\n",
    "    \"\"\"SVD reconstruction for k components using cuda\n",
    "    \n",
    "    Inputs:\n",
    "    u (m,n): array\n",
    "    s (n): array (diagonal matrix)\n",
    "    vt (n,n): array\n",
    "    k int: number of reconstructed singular components\n",
    "    y (m,n): output array\n",
    "    \"\"\"\n",
    "    m, n = cuda.grid(2)\n",
    "\n",
    "    if m >= u.shape[0] or n >= vt.shape[0]:\n",
    "        return\n",
    "\n",
    "    element = 0.0\n",
    "    for p in range(k):\n",
    "        element += u[m, p] * s[p] * vt[p, n]\n",
    "\n",
    "    y[m, n] = element\n",
    "\n",
    "def calculate(u: np.ndarray, s: np.ndarray, vt: np.ndarray, k: int, block_size:tuple=(32,32)) -> np.ndarray:\n",
    "    \"\"\"Host function to perform SVD reconstruction using CUDA kernel.\n",
    "\n",
    "    Args:\n",
    "        u (np.ndarray): Left singular vectors, shape (m, n).\n",
    "        s (np.ndarray): Singular values, shape (n,).\n",
    "        vt (np.ndarray): Right singular vectors, shape (n, n).\n",
    "        k (int): Number of singular components to use in reconstruction.\n",
    "        block_size (tuple(int,int)): Number of threads per block\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reconstructed matrix, shape (m, n).\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert to correct dtype\n",
    "    u = u.astype(np.float32)\n",
    "    s = s.astype(np.float32)\n",
    "    vt = np.asfortranarray(vt.astype(np.float32))\n",
    "\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        # Ensure inputs are in the correct dtype and order\n",
    "        u = cuda.to_device(u)\n",
    "        s = cuda.to_device(s)\n",
    "        vt = cuda.to_device(vt)\n",
    "        \n",
    "        # create array where results are stored. Also pin that array so no data gets moved out of the ram.\n",
    "        m, n = u.shape[0], vt.shape[1]\n",
    "        y = cuda.device_array((m, n), dtype=np.float32)\n",
    "        y_ret = cuda.pinned_array_like(y)\n",
    "\n",
    "    # Define CUDA thread and block dimensions\n",
    "    blocks_per_grid_x = (m + block_size[0] - 1) // block_size[0]\n",
    "    blocks_per_grid_y = (n + block_size[1] - 1) // block_size[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        # Launch the CUDA kernel\n",
    "        svd_reco_kernel[blocks_per_grid, block_size](u, s, vt, k, y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        # copy back to host\n",
    "        y.copy_to_host(y_ret)\n",
    "\n",
    "    # calculate number of transfers done in each thread\n",
    "    num_transfers_per_thread = 1 + (4*k) + 1\n",
    "    number_of_GB_transferred = 1e-9*4*num_transfers_per_thread*y.shape[0]*y.shape[1]\n",
    "\n",
    "    print(f\"Cuda transfer overhead: {t_xfer.elapsed_time()*1000}ms\")\n",
    "    print(f\"Cuda kernel time: {t_kernel.elapsed_time()*1000}ms\")\n",
    "    print(f\"Consumed memory bandwidth: {number_of_GB_transferred / t_kernel.elapsed_time()} GB/s\")\n",
    "\n",
    "    return y_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda transfer overhead: 1.002495974302292ms\n",
      "Cuda kernel time: 0.5836799740791321ms\n",
      "Consumed memory bandwidth: 119.89474216655664 GB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04974082,  0.00398783,  0.06063343, ...,  0.06974227,\n",
       "         0.06019334, -0.04235162],\n",
       "       [ 0.02688764,  0.00190527,  0.02418954, ...,  0.03627283,\n",
       "         0.01780313, -0.00637781],\n",
       "       [ 0.0290446 , -0.02647049, -0.03107073, ...,  0.04624368,\n",
       "         0.00306286,  0.03151328],\n",
       "       ...,\n",
       "       [ 1.3522642 , -0.00273046, -0.623177  , ..., -0.6813155 ,\n",
       "        -0.4641527 , -0.96072966],\n",
       "       [ 0.2960358 ,  0.5249512 ,  0.0184323 , ..., -0.19332825,\n",
       "         0.5000806 ,  1.5635303 ],\n",
       "       [ 0.9867547 , -0.9557681 ,  1.4895947 , ...,  1.0953    ,\n",
       "        -0.5926281 ,  1.8346974 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "subfolder = '001'\n",
    "folders = os.path.join('adni_png', subfolder)\n",
    "\n",
    "# Get all PNGs from 001 with 145 in the name\n",
    "files = sorted(glob.glob(f\"{folders}/*145.png\"))\n",
    "\n",
    "# Load all images using ImageIO and create a numpy array from them\n",
    "images = np.array([imageio.imread(f) for f in files])\n",
    "\n",
    "# Get all the names of the files\n",
    "names = [f[-17:-4] for f in files]\n",
    "\n",
    "im = images[0]\n",
    "im = im - im.min() / im.max() - im.min()  # normalize image\n",
    "u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "# convert to correct dtype\n",
    "u = u.astype(np.float32)\n",
    "s = s.astype(np.float32)\n",
    "vt = vt.astype(np.float32)\n",
    "\n",
    "calculate(u, s, vt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.48357570e-04,  6.87303161e-03,  5.32832742e-03, ...,\n",
       "         8.10194574e-03,  5.62756276e-03,  1.05523104e-02],\n",
       "       [ 5.10228332e-04, -5.43693895e-04,  8.77237471e-05, ...,\n",
       "         2.47143582e-03, -7.51565312e-05,  2.96106702e-03],\n",
       "       [ 3.63633828e-03,  3.92481964e-03,  1.46562816e-03, ...,\n",
       "         7.42339715e-03,  2.51593534e-03,  1.07109863e-02],\n",
       "       ...,\n",
       "       [ 1.30805051e+00,  1.37908542e+00,  1.05979729e+00, ...,\n",
       "         6.39316261e-01,  1.12439133e-01,  8.58197808e-02],\n",
       "       [ 1.23633599e+00,  1.39993715e+00,  1.02188718e+00, ...,\n",
       "         6.65664971e-01,  9.83572006e-02,  2.19427586e-01],\n",
       "       [ 1.16466701e+00,  1.40309119e+00,  9.09437180e-01, ...,\n",
       "         3.75059962e-01,  1.99167326e-01,  4.85913455e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reconstruct_svd_for_loops3(u,s,vt,k):\n",
    "    \"\"\"SVD reconstruction for k components using 3 for-loops\n",
    "    \n",
    "    Inputs:\n",
    "    u: (m,n) numpy array\n",
    "    s: (n) numpy array (diagonal matrix)\n",
    "    vt: (n,n) numpy array\n",
    "    k: number of reconstructed singular components\n",
    "    \n",
    "    Ouput:\n",
    "    (m,n) numpy array U_mk * S_k * V^T_nk for k reconstructed components\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    if k is None:\n",
    "        k = min(u.shape[0], vt.shape[0])\n",
    "\n",
    "    reco = np.zeros((u.shape[0], vt.shape[0]))\n",
    "\n",
    "    for m in range(u.shape[0]):\n",
    "        for n in range(vt.shape[0]):\n",
    "            element = 0\n",
    "            for p in range(k):\n",
    "                element += u[m, p]*s[p]*vt[p, n]\n",
    "\n",
    "            reco[m, n] = element\n",
    "            \n",
    "    ### END SOLUTION\n",
    "\n",
    "    return reco\n",
    "\n",
    "reconstruct_svd_for_loops3(u, s, vt, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-bcMvFOTC-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
