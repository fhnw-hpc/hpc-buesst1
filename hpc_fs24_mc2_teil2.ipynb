{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"numba\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 48 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cuda': {'time_transfer_ms': 1.4065919443964958,\n",
       "  'time_kernel_ms': 0.0798719972372055,\n",
       "  'consumed_mem_bandwidth_GB/s': 1484.2308205707225,\n",
       "  'consumed GFLOPs': 277.8846249967212},\n",
       " 'cpu': {'time_reco_ms': 0.0,\n",
       "  'consumed_mem_bandwidth_GB/s': inf,\n",
       "  'consumed GFLOPs': inf},\n",
       " 'cpu_cuda_same': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cuda_kernel import svd_reco_cuda_perfmeasure\n",
    "import os\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "subfolder = '001'\n",
    "folders = os.path.join('adni_png', subfolder)\n",
    "\n",
    "# Get all PNGs from 001 with 145 in the name\n",
    "files = sorted(glob.glob(f\"{folders}/*145.png\"))\n",
    "\n",
    "# Load all images using ImageIO and create a numpy array from them\n",
    "images = np.array([imageio.imread(f) for f in files])\n",
    "\n",
    "# Get all the names of the files\n",
    "names = [f[-17:-4] for f in files]\n",
    "\n",
    "im = images[0]\n",
    "im = im - im.min() / im.max() - im.min()  # normalize image\n",
    "u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "svd_reco_cuda_perfmeasure(u, s, vt, len(s), (32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:59<00:00,  5.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "matrix_sizes = [int(size) for size in np.linspace(20, 5000, 10).astype(int)]\n",
    "block_sizes_warpfriendly = [8, 16, 32]\n",
    "\n",
    "results = []\n",
    "for matrix_size in tqdm(matrix_sizes):\n",
    "    # random image\n",
    "    im = np.random.normal(size=(matrix_size, matrix_size))\n",
    "\n",
    "    # normalize image\n",
    "    im = im - im.min() / im.max() - im.min()  \n",
    "\n",
    "    # decomposition\n",
    "    u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "    # reconstruct with different block sizes\n",
    "    for block_size in block_sizes_warpfriendly:\n",
    "        result = pd.json_normalize(svd_reco_cuda_perfmeasure(u, s, vt, len(s), (block_size, block_size)))\n",
    "        result[[\"matrix_size\", \"block_size\"]] = [[matrix_size, block_size]]\n",
    "        results.append(result)\n",
    "        \n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cpu_cuda_same</th>\n",
       "      <th>cuda.time_transfer_ms</th>\n",
       "      <th>cuda.time_kernel_ms</th>\n",
       "      <th>cuda.consumed_mem_bandwidth_GB/s</th>\n",
       "      <th>cuda.consumed GFLOPs</th>\n",
       "      <th>cpu.time_reco_ms</th>\n",
       "      <th>cpu.consumed_mem_bandwidth_GB/s</th>\n",
       "      <th>cpu.consumed GFLOPs</th>\n",
       "      <th>matrix_size</th>\n",
       "      <th>block_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.955392</td>\n",
       "      <td>0.030720</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.371712</td>\n",
       "      <td>0.040960</td>\n",
       "      <td>3.164063</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.357376</td>\n",
       "      <td>0.082944</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.289352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.421504</td>\n",
       "      <td>8.213504</td>\n",
       "      <td>366.644205</td>\n",
       "      <td>68.715808</td>\n",
       "      <td>2.002478</td>\n",
       "      <td>1503.853785</td>\n",
       "      <td>281.849614</td>\n",
       "      <td>573</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>11.082752</td>\n",
       "      <td>8.482816</td>\n",
       "      <td>355.004008</td>\n",
       "      <td>66.534223</td>\n",
       "      <td>3.509521</td>\n",
       "      <td>858.075268</td>\n",
       "      <td>160.818947</td>\n",
       "      <td>573</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.744064</td>\n",
       "      <td>9.347072</td>\n",
       "      <td>322.179363</td>\n",
       "      <td>60.382286</td>\n",
       "      <td>4.034281</td>\n",
       "      <td>746.461081</td>\n",
       "      <td>139.900414</td>\n",
       "      <td>573</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>24.981504</td>\n",
       "      <td>161.689606</td>\n",
       "      <td>141.302376</td>\n",
       "      <td>26.488315</td>\n",
       "      <td>12.001038</td>\n",
       "      <td>1903.762515</td>\n",
       "      <td>356.876236</td>\n",
       "      <td>1126</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.495200</td>\n",
       "      <td>4.888288</td>\n",
       "      <td>4673.850113</td>\n",
       "      <td>876.152369</td>\n",
       "      <td>15.949488</td>\n",
       "      <td>1432.467674</td>\n",
       "      <td>268.528069</td>\n",
       "      <td>1126</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>6.316032</td>\n",
       "      <td>4.961280</td>\n",
       "      <td>4605.087018</td>\n",
       "      <td>863.262150</td>\n",
       "      <td>12.998104</td>\n",
       "      <td>1757.727539</td>\n",
       "      <td>329.500756</td>\n",
       "      <td>1126</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>13.137920</td>\n",
       "      <td>18.454529</td>\n",
       "      <td>4111.587047</td>\n",
       "      <td>770.807868</td>\n",
       "      <td>30.027866</td>\n",
       "      <td>2526.899537</td>\n",
       "      <td>473.723169</td>\n",
       "      <td>1680</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>14.159136</td>\n",
       "      <td>16.407553</td>\n",
       "      <td>4624.541082</td>\n",
       "      <td>866.972439</td>\n",
       "      <td>30.002117</td>\n",
       "      <td>2529.068239</td>\n",
       "      <td>474.129740</td>\n",
       "      <td>1680</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>13.050816</td>\n",
       "      <td>19.125248</td>\n",
       "      <td>3967.394398</td>\n",
       "      <td>743.775769</td>\n",
       "      <td>30.000210</td>\n",
       "      <td>2529.229032</td>\n",
       "      <td>474.159884</td>\n",
       "      <td>1680</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>24.028160</td>\n",
       "      <td>103.417763</td>\n",
       "      <td>1722.818922</td>\n",
       "      <td>322.992387</td>\n",
       "      <td>59.000492</td>\n",
       "      <td>3019.806653</td>\n",
       "      <td>566.150363</td>\n",
       "      <td>2233</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>22.212768</td>\n",
       "      <td>107.619331</td>\n",
       "      <td>1655.558312</td>\n",
       "      <td>310.382434</td>\n",
       "      <td>55.001020</td>\n",
       "      <td>3239.395872</td>\n",
       "      <td>607.318732</td>\n",
       "      <td>2233</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>22.178816</td>\n",
       "      <td>72.625404</td>\n",
       "      <td>2453.274858</td>\n",
       "      <td>459.937543</td>\n",
       "      <td>66.003084</td>\n",
       "      <td>2699.420501</td>\n",
       "      <td>506.084684</td>\n",
       "      <td>2233</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>122.725378</td>\n",
       "      <td>312.320007</td>\n",
       "      <td>1107.904897</td>\n",
       "      <td>207.713529</td>\n",
       "      <td>94.999552</td>\n",
       "      <td>3642.342087</td>\n",
       "      <td>682.877864</td>\n",
       "      <td>2786</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>34.130944</td>\n",
       "      <td>76.967812</td>\n",
       "      <td>4495.656802</td>\n",
       "      <td>842.860017</td>\n",
       "      <td>97.002029</td>\n",
       "      <td>3567.150788</td>\n",
       "      <td>668.780760</td>\n",
       "      <td>2786</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>33.844991</td>\n",
       "      <td>75.982590</td>\n",
       "      <td>4553.949358</td>\n",
       "      <td>853.788890</td>\n",
       "      <td>94.001293</td>\n",
       "      <td>3681.022398</td>\n",
       "      <td>690.129771</td>\n",
       "      <td>2786</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>179.342145</td>\n",
       "      <td>285.190460</td>\n",
       "      <td>2090.532362</td>\n",
       "      <td>391.945481</td>\n",
       "      <td>157.999516</td>\n",
       "      <td>3773.428573</td>\n",
       "      <td>707.464903</td>\n",
       "      <td>3340</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>49.931010</td>\n",
       "      <td>127.951965</td>\n",
       "      <td>4659.560210</td>\n",
       "      <td>873.602150</td>\n",
       "      <td>157.000065</td>\n",
       "      <td>3797.449937</td>\n",
       "      <td>711.968572</td>\n",
       "      <td>3340</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>47.976447</td>\n",
       "      <td>127.364128</td>\n",
       "      <td>4681.065974</td>\n",
       "      <td>877.634179</td>\n",
       "      <td>153.001547</td>\n",
       "      <td>3896.691887</td>\n",
       "      <td>730.575045</td>\n",
       "      <td>3340</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>239.959032</td>\n",
       "      <td>278.439941</td>\n",
       "      <td>3390.545280</td>\n",
       "      <td>635.686418</td>\n",
       "      <td>234.000206</td>\n",
       "      <td>4034.454692</td>\n",
       "      <td>756.411680</td>\n",
       "      <td>3893</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>66.694146</td>\n",
       "      <td>203.635773</td>\n",
       "      <td>4636.038239</td>\n",
       "      <td>869.201352</td>\n",
       "      <td>232.907057</td>\n",
       "      <td>4053.390404</td>\n",
       "      <td>759.961898</td>\n",
       "      <td>3893</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>70.393186</td>\n",
       "      <td>203.565414</td>\n",
       "      <td>4637.640592</td>\n",
       "      <td>869.501774</td>\n",
       "      <td>231.536388</td>\n",
       "      <td>4077.386002</td>\n",
       "      <td>764.460783</td>\n",
       "      <td>3893</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>300.469239</td>\n",
       "      <td>372.568054</td>\n",
       "      <td>3774.393398</td>\n",
       "      <td>707.658970</td>\n",
       "      <td>329.000235</td>\n",
       "      <td>4274.217026</td>\n",
       "      <td>801.370631</td>\n",
       "      <td>4446</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>93.152256</td>\n",
       "      <td>303.003418</td>\n",
       "      <td>4640.932481</td>\n",
       "      <td>870.125913</td>\n",
       "      <td>324.000120</td>\n",
       "      <td>4340.178650</td>\n",
       "      <td>813.737740</td>\n",
       "      <td>4446</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>98.260989</td>\n",
       "      <td>301.262024</td>\n",
       "      <td>4667.758604</td>\n",
       "      <td>875.155528</td>\n",
       "      <td>326.000690</td>\n",
       "      <td>4313.544251</td>\n",
       "      <td>808.744071</td>\n",
       "      <td>4446</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>342.880420</td>\n",
       "      <td>551.151917</td>\n",
       "      <td>3628.945015</td>\n",
       "      <td>680.393171</td>\n",
       "      <td>458.147287</td>\n",
       "      <td>4365.626634</td>\n",
       "      <td>818.514068</td>\n",
       "      <td>5000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>108.470269</td>\n",
       "      <td>428.733429</td>\n",
       "      <td>4665.136574</td>\n",
       "      <td>874.669374</td>\n",
       "      <td>459.000349</td>\n",
       "      <td>4357.513026</td>\n",
       "      <td>816.992843</td>\n",
       "      <td>5000</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>108.203007</td>\n",
       "      <td>427.419006</td>\n",
       "      <td>4679.483061</td>\n",
       "      <td>877.359206</td>\n",
       "      <td>471.999884</td>\n",
       "      <td>4237.501045</td>\n",
       "      <td>794.491721</td>\n",
       "      <td>5000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cpu_cuda_same  cuda.time_transfer_ms  cuda.time_kernel_ms  \\\n",
       "0           True               0.955392             0.030720   \n",
       "0           True               0.371712             0.040960   \n",
       "0           True               0.357376             0.082944   \n",
       "0           True               6.421504             8.213504   \n",
       "0           True              11.082752             8.482816   \n",
       "0           True               6.744064             9.347072   \n",
       "0           True              24.981504           161.689606   \n",
       "0           True               6.495200             4.888288   \n",
       "0           True               6.316032             4.961280   \n",
       "0           True              13.137920            18.454529   \n",
       "0           True              14.159136            16.407553   \n",
       "0           True              13.050816            19.125248   \n",
       "0           True              24.028160           103.417763   \n",
       "0           True              22.212768           107.619331   \n",
       "0           True              22.178816            72.625404   \n",
       "0           True             122.725378           312.320007   \n",
       "0           True              34.130944            76.967812   \n",
       "0           True              33.844991            75.982590   \n",
       "0           True             179.342145           285.190460   \n",
       "0           True              49.931010           127.951965   \n",
       "0           True              47.976447           127.364128   \n",
       "0           True             239.959032           278.439941   \n",
       "0           True              66.694146           203.635773   \n",
       "0           True              70.393186           203.565414   \n",
       "0           True             300.469239           372.568054   \n",
       "0           True              93.152256           303.003418   \n",
       "0           True              98.260989           301.262024   \n",
       "0           True             342.880420           551.151917   \n",
       "0           True             108.470269           428.733429   \n",
       "0           True             108.203007           427.419006   \n",
       "\n",
       "   cuda.consumed_mem_bandwidth_GB/s  cuda.consumed GFLOPs  cpu.time_reco_ms  \\\n",
       "0                          4.218750              0.781250          0.000000   \n",
       "0                          3.164063              0.585938          0.000000   \n",
       "0                          1.562500              0.289352          0.000000   \n",
       "0                        366.644205             68.715808          2.002478   \n",
       "0                        355.004008             66.534223          3.509521   \n",
       "0                        322.179363             60.382286          4.034281   \n",
       "0                        141.302376             26.488315         12.001038   \n",
       "0                       4673.850113            876.152369         15.949488   \n",
       "0                       4605.087018            863.262150         12.998104   \n",
       "0                       4111.587047            770.807868         30.027866   \n",
       "0                       4624.541082            866.972439         30.002117   \n",
       "0                       3967.394398            743.775769         30.000210   \n",
       "0                       1722.818922            322.992387         59.000492   \n",
       "0                       1655.558312            310.382434         55.001020   \n",
       "0                       2453.274858            459.937543         66.003084   \n",
       "0                       1107.904897            207.713529         94.999552   \n",
       "0                       4495.656802            842.860017         97.002029   \n",
       "0                       4553.949358            853.788890         94.001293   \n",
       "0                       2090.532362            391.945481        157.999516   \n",
       "0                       4659.560210            873.602150        157.000065   \n",
       "0                       4681.065974            877.634179        153.001547   \n",
       "0                       3390.545280            635.686418        234.000206   \n",
       "0                       4636.038239            869.201352        232.907057   \n",
       "0                       4637.640592            869.501774        231.536388   \n",
       "0                       3774.393398            707.658970        329.000235   \n",
       "0                       4640.932481            870.125913        324.000120   \n",
       "0                       4667.758604            875.155528        326.000690   \n",
       "0                       3628.945015            680.393171        458.147287   \n",
       "0                       4665.136574            874.669374        459.000349   \n",
       "0                       4679.483061            877.359206        471.999884   \n",
       "\n",
       "   cpu.consumed_mem_bandwidth_GB/s  cpu.consumed GFLOPs  matrix_size  \\\n",
       "0                              inf                  inf           20   \n",
       "0                              inf                  inf           20   \n",
       "0                              inf                  inf           20   \n",
       "0                      1503.853785           281.849614          573   \n",
       "0                       858.075268           160.818947          573   \n",
       "0                       746.461081           139.900414          573   \n",
       "0                      1903.762515           356.876236         1126   \n",
       "0                      1432.467674           268.528069         1126   \n",
       "0                      1757.727539           329.500756         1126   \n",
       "0                      2526.899537           473.723169         1680   \n",
       "0                      2529.068239           474.129740         1680   \n",
       "0                      2529.229032           474.159884         1680   \n",
       "0                      3019.806653           566.150363         2233   \n",
       "0                      3239.395872           607.318732         2233   \n",
       "0                      2699.420501           506.084684         2233   \n",
       "0                      3642.342087           682.877864         2786   \n",
       "0                      3567.150788           668.780760         2786   \n",
       "0                      3681.022398           690.129771         2786   \n",
       "0                      3773.428573           707.464903         3340   \n",
       "0                      3797.449937           711.968572         3340   \n",
       "0                      3896.691887           730.575045         3340   \n",
       "0                      4034.454692           756.411680         3893   \n",
       "0                      4053.390404           759.961898         3893   \n",
       "0                      4077.386002           764.460783         3893   \n",
       "0                      4274.217026           801.370631         4446   \n",
       "0                      4340.178650           813.737740         4446   \n",
       "0                      4313.544251           808.744071         4446   \n",
       "0                      4365.626634           818.514068         5000   \n",
       "0                      4357.513026           816.992843         5000   \n",
       "0                      4237.501045           794.491721         5000   \n",
       "\n",
       "   block_size  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  \n",
       "0           8  \n",
       "0          16  \n",
       "0          32  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-d94spYeM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
