{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"numba\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU-basierte SVD-Rekonstruktion\n",
    "\n",
    "Die Aufgabe besteht darin, eine Singularwertzerlegung (SVD) effizient auf der GPU zu implementieren, um eine Matrix zu rekonstruieren. Hierbei wurde die Berechnung der einzelnen Elemente der Zielmatrix auf der GPU mittels eines CUDA-Kernels ausgelagert, während Steuerlogik und Speicheroperationen auf der CPU verbleiben. Meine Implementierung basiert vollständig auf Numba.\n",
    "\n",
    "#### Rekonstruktionsstrategie\n",
    "\n",
    "Die Rekonstruktion einer Matrix $Y$ aus den SVD-Komponenten $U$, $S$ und $V^T$ basiert auf der Formel:\n",
    "\n",
    "$$\n",
    "Y_{m,n} = \\sum_{p=1}^{k} U_{m,p} \\cdot S_p \\cdot V^T_{p,n}\n",
    "$$\n",
    "\n",
    "Da $S$ eine Diagonalmatrix ist, kann jeder Eintrag $Y_{m,n}$ unabhängig berechnet werden, indem über die ersten $k$ Singulärwerte iteriert wird. Diese Unabhängigkeit erlaubt eine effiziente Parallelisierung auf der GPU, bei der jedes Element $Y_{m,n}$ gleichzeitig von einem eigenen Thread berechnet wird.\n",
    "\n",
    "In den ersten Experimenten nutze ich nur das global memory (kein shared). Die nachfolgenden Annahmen treffen aber auch auf den Kernel mit shared memory zu.\n",
    "\n",
    "#### Designentscheidungen & Annahmen\n",
    "\n",
    "Eine NVIDIA-GPU besteht aus mehreren **Streaming Multiprocessors (SMs)**, die die parallele Berechnung ermöglichen. Die von mir verwendete NVIDIA RTX 4090 verfügt über 128 SMs. Jeder SM enthält:\n",
    "- **CUDA-Kerne**, die elementare Operationen ausführen,\n",
    "- **Register**, die Threads für schnelle Berechnungen (durch schnelleren Memoryzugriff als global Memory) nutzen können,\n",
    "- **Shared Memory**, der von Threads eines Blocks gemeinsam genutzt werden kann (schnelleres Memory als global Memory, kann von mehreren Threads gemeinsam genutzt werden),\n",
    "- **L1- und L2-Caches**, die Speicherzugriffe beschleunigen (habe ich als Programmierer keinen direkten Einfluss),\n",
    "- sowie **Warp-Scheduler**, die Threads in Gruppen, den sogenannten **Warps**, steuern.\n",
    "\n",
    "Ein Warp besteht aus **32 Threads**, die nach dem **Single Instruction Multiple Data (SIMD)**-Prinzip arbeiten. Das bedeutet, alle Threads eines Warps führen die gleiche Instruktion aus, jedoch auf unterschiedlichen Daten. Divergieren Threads in ihrem Kontrollfluss (z. B. durch if-else-Konstrukte), müssen diese nacheinander ausgeführt werden, was die Effizienz des Warps verringert. Dies wird als **Warp-Divergenz** bezeichnet und sollte bei der Kernel-Implementierung vermieden werden.\n",
    "\n",
    "Deshalb habe ich meinen Kernel so simpel wie möglich designed, wobei die Warp-Divergenz nur bei Threads auftreten kann, welche sich ausserhalb der Zielmatrix befindet (globale Threadid ausserhalb der Zielmatrix).\n",
    "\n",
    "#### Blöcke, Threads und Speicherzugriff\n",
    "\n",
    "Ein **Block** ist eine Sammlung von Threads, die auf einem SM ausgeführt werden. Threads innerhalb eines Blocks können über den **Shared Memory** miteinander kommunizieren, was wesentlich schneller ist als Zugriffe auf den **Global Memory**, der für die gesamte GPU zugänglich ist. Der **Global Memory** hat eine hohe Latenz, daher ist es wichtig, dass Daten, die häufig oder von mehreren Threads benötigt werden, in den Shared Memory verschoben werden. In meinem Fall wäre dies die Diagonalmatrix $S$, welche für jedes Element der Zielmatrix benötigt wird. Im 5.2.2 habe ich diese Verbesserung implementiert.\n",
    "\n",
    "Die Effizienz des **Global Memory-Zugriffs** hängt von der Anordnung der Daten ab:\n",
    "- **Memory Coalescing:** Wenn Threads eines Warps auf aufeinanderfolgende Speicheradressen zugreifen, können diese Daten in einem einzigen Ladevorgang übertragen werden.\n",
    "- **Strided oder random access:** Zugriffe mit unregelmässigen Adressmustern führen zu mehrfachen Speicherzugriffen und reduzieren die Effizienz erheblich.\n",
    "\n",
    "Die Grösse eines Blocks beeinflusst, wie viele Blöcke gleichzeitig auf einem SM ausgeführt werden können. Zu grosse Blöcke können die Anzahl gleichzeitig laufender Blöcke reduzieren, da die verfügbaren Ressourcen (Register, Shared Memory) eines SMs begrenzt sind. Eine sorgfältige Wahl der Blockgrösse ist daher entscheidend, um die Auslastung der GPU zu maximieren.\n",
    "\n",
    "Ein paar erste Versuche haben gezeigt, dass die Berechnung der Rekonstruktion am schnellsten ist, wenn alle Inputs in den Kernel row-major sind. Die Ergebnisse habe ich allerdings nirgends aufgezeigt, da mein Code ansonnsten sehr überladen würde. Deshalb sind alle Kernelparameter in meinen Experimenten row-major.\n",
    "\n",
    "#### Bank Conflicts im Shared Memory\n",
    "\n",
    "Der **Shared Memory** eines SMs ist in 32 **Speicherbänke** unterteilt, die parallelen Zugriff ermöglichen. Jeder Speicherbank ist ein bestimmter Adressbereich zugeordnet. Ein **Bank Conflict** tritt auf, wenn mehrere Threads eines Warps gleichzeitig auf Adressen zugreifen, die zur gleichen Speicherbank gehören. In diesem Fall müssen die Zugriffe seriell ausgeführt werden, was die Parallelität innerhalb des Warps reduziert.\n",
    "\n",
    "Bank Conflicts können durch geschickte Datenanordnung und Padding vermieden werden. Dies stellt sicher, dass Threads auf unterschiedliche Speicherbänke zugreifen können, um die maximale Parallelität des Shared Memory zu nutzen.\n",
    "\n",
    "Um bank-conflicts zu vermeiden habe ich im shared memory kernel die tile-size auf 32 begrenzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_k_timings, make_reconstructor, random_svd, reconstruct_svd_broadcast_timeit\n",
    "from src.kernels.global_mem import fp32 as kernel_globalmem_fp32\n",
    "from src.kernels.global_mem import fp64 as kernel_globalmem_fp64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code wurde modularisiert, um die Übersichtlichkeit und Wiederverwendbarkeit zu gewährleisten. Zwei Varianten des CUDA-Kernels wurden implementiert: eine 64-Bit-Version (fp64) für höhere Genauigkeit und eine 32-Bit-Version (fp32) für schnellere Ausführung. Beide Varianten arbeiten ausschliesslich mit globalem Speicher. In einer späteren Version wird der Kernel zusätzlich mit Shared Memory optimiert, um die Effizienz weiter zu steigern.\n",
    "\n",
    "Die Funktion `make_reconstructor` generiert eine rekonfigurierbare Funktion zur SVD-Rekonstruktion basierend auf dem übergebenen CUDA-Kernel. Sie bietet folgende Parameter:\n",
    "\n",
    "- **kernel (callable):** Der CUDA-Kernel zur Rekonstruktion.\n",
    "- **block_size (tuple):** Die Grösse der CUDA-Thread-Blöcke (Threads pro Block).\n",
    "- **pin_memory (bool):** Option, ob für das Ausgabe-Array gepinnter Speicher verwendet werden soll (beschleunigt den Datenfluss zwischen GPU und Host). Standardmässig deaktiviert.\n",
    "- **timeit (bool):** Gibt an, ob Laufzeiten gemessen und zurückgegeben werden sollen. Standardmässig deaktiviert.\n",
    "\n",
    "Basierend auf der Kernel-Signatur werden automatisch die benötigten Datentypen und die Speicheranordnung (Row- oder Column-Major) der Eingabedaten abgeleitet. Dadurch wird sichergestellt, dass die Daten korrekt auf die GPU übertragen und verarbeitet werden. Optional kann auch das Ausgabe-Array in gepinntem Speicher bereitgestellt werden, um den Datentransfer von der GPU zum Host zu beschleunigen.\n",
    "\n",
    "Die Funktion unterstützt damit eine flexible und effiziente SVD-Rekonstruktion und erlaubt präzise Vergleiche zwischen verschiedenen Konfigurationen, wie z.B. Genauigkeit (fp64) und Geschwindigkeit (fp32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = random_svd((5000, 5000))\n",
    "k = len(s)\n",
    "\n",
    "cpu_reconstruction, time_cpu = reconstruct_svd_broadcast_timeit(u, s, vt, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich erstelle zuerst zufällige Matrizen, welche sich zu einer 5000x5000 Matrix rekonstruieren lassen. Als k verwende ich die Originalgrösse, damit am meisten Berchnungen durchgeführt werden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_func = make_reconstructor(kernel_globalmem_fp64, (4, 8), timeit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun erstelle ich eine Rekonstruktionsfunktion, welche den fp64 Kernel nutzt mit einer Blockgrösse von (4x8). Dieser komplette Block sollte theoretisch auf einem einzelnen Warp ausgeführt werden können, weil 4x8 = 32 = num threads per Warp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_result, time_gpu = reco_func(u,s,vt,k)\n",
    "\n",
    "np.allclose(cpu_reconstruction, gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie unschwer zu erkennen ist, hat die Rekonstruktion funktioniert. Die GPU liefert das gleiche Resultat wie wie CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 0.45200014114379883}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'h2d': 0.267315185546875,\n",
       " 'd_maloc_y': 3.0720001086592673e-06,\n",
       " 'h_maloc_y': 0.0424317741394043,\n",
       " 'kernel': 0.4063907775878906,\n",
       " 'd2h': 0.03409503936767578,\n",
       " 'mem_operations_total': 0.34384507105406376,\n",
       " 'total': 0.7502358486419544}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"CPU Time:\")\n",
    "display(time_cpu)\n",
    "\n",
    "print(\"GPU Time:\")\n",
    "display(time_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sind nun verschiedene Zeitmessungen zu erkennen.\n",
    "\n",
    "Bei der CPU gibt es nur die totale Rekonstruktionszeit.\n",
    "\n",
    "Bei der GPU wurden noch ein paar weitere Metriken, wie:\n",
    "\n",
    "- *h2d*: Zeit in Sekunden, welche benötigt wird die Daten von der CPU zur GPU zu senden\n",
    "- *d_maloc_y*: Zeit in Sekunden, welche benötigt wird die output Matrix auf der GPU zu alloziieren\n",
    "- *h_maloc_y*: Zeit in Sekunden, welche benötigt wird die output Matrix auf der CPU zu alloziieren\n",
    "- *kernel*: Kernel execution time in Sekunden\n",
    "- *d2h*: Zeit in Sekunden welche benötigt wird die Daten von der GPU auf die CPU zu senden\n",
    "- *mem_operations_total*: h2d + d_maloc_y + h_maloc_y + d2h\n",
    "- *total*: Totale Zeit in Sekunden\n",
    "\n",
    "Wie man erkennen kann, benötigt die GPU total mehr Zeit als die CPU. Die Kernel execution time ist allerdings etwas kleiner als die totale Zeit der Berechnung der CPU. Das bedeutet also, dass die Berechnung auf der GPU schneller läuft, allerdings benötigt die Übertragung der Daten zur GPU auch einiges an Zeit, was schlussendlich zu einer langsameren totalen Zeit führt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco_func = make_reconstructor(kernel_globalmem_fp32, (4, 8), timeit=True)\n",
    "gpu_result, time_gpu = reco_func(u,s,vt,k)\n",
    "\n",
    "np.allclose(cpu_reconstruction, gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diesmal habe ich den 32-Bit-Kernel verwendet, da dieser theoretisch deutlich schneller sein sollte als die 64-Bit-Version. GPUs sind bei 32-Bit-Operationen erheblich effizienter. Laut NVIDIA liegt der Durchsatz bei der RTX 4090 bei 64-Bit-Operationen um das 64-Fache unter dem von 32-Bit-Operationen.\n",
    "\n",
    "Was hier auch deutlich wird ist die Präzision. np.allclose gibt nun ein False zurück. Dies deutet darauf hin, dass die Rekonstruktion nicht mehr die Genauigkeit besitzt wie die 64 Bit Rekonstruktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.54465679, -69.25551935,   9.01122561,  -2.74354276],\n",
       "       [-26.49204659, -10.10723365,  18.9068482 , -59.96872846],\n",
       "       [  4.76031669,  82.05960411, -53.75030988,   8.44785843],\n",
       "       [ -3.90419945,  18.33388695,  26.67122977,   6.13584378]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_reconstruction[:4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.544616 , -69.25548  ,   9.011216 ,  -2.7434847],\n",
       "       [-26.492111 , -10.107256 ,  18.906857 , -59.968765 ],\n",
       "       [  4.7603397,  82.05929  , -53.75032  ,   8.447853 ],\n",
       "       [ -3.9041739,  18.3339   ,  26.671164 ,   6.1358523]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_result[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleicht man die ersten 16 Elemente der Rekonstruktion, so kann man sehen, dass die Rektionstruktion auf ca 3 Stellen nach dem Komma genau ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total': 0.45200014114379883}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'h2d': 0.13977293395996093,\n",
       " 'd_maloc_y': 4.0959999896585945e-06,\n",
       " 'h_maloc_y': 0.07301734161376953,\n",
       " 'kernel': 0.2958124694824219,\n",
       " 'd2h': 0.017441696166992187,\n",
       " 'mem_operations_total': 0.23023606774071229,\n",
       " 'total': 0.5260485372231342}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"CPU Time:\")\n",
    "display(time_cpu)\n",
    "\n",
    "print(\"GPU Time:\")\n",
    "display(time_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier zeigt sich, dass sowohl die Kernel-Execution-Time als auch die Transferzeiten von Host zu Device (h2d) und Device zu Host (d2h) bei der 32-Bit-Rekonstruktion kürzer sind als bei der 64-Bit-Version. Dies ist auf zwei Faktoren zurückzuführen: Zum einen sind die Rechenoperationen bei 32 Bit schneller, und zum anderen wird nur die halbe Datenmenge übertragen, was den Datentransfer beschleunigt.\n",
    "\n",
    "Die Kernel-Execution-Time ist jedoch bei weitem nicht 64-mal kürzer, was darauf hindeutet, dass die Rekonstruktion primär Memory-Bound ist, also durch den Zugriff auf den Speicher limitiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrixgrössen\n",
    "\n",
    "5000x5000\n",
    "\n",
    "25000x1000\n",
    "\n",
    "1000x25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h2d': 0.09527085113525391,\n",
       " 'd_maloc_y': 1.0239999974146486e-06,\n",
       " 'h_maloc_y': 0.00013312000036239626,\n",
       " 'kernel': 1.5263999812304975e-05,\n",
       " 'd2h': 0.00012697599828243256,\n",
       " 'mem_operations_total': 0.09553197113389615,\n",
       " 'total': 0.09554723513370846}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = random_svd((10000, 10))\n",
    "k = len(s)\n",
    "\n",
    "reco_func = make_reconstructor(kernel_globalmem_fp64, (1, 32), timeit=True)\n",
    "reco_func(u, s, vt, k)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h2d': 0.2670059509277344,\n",
       " 'd_maloc_y': 2.0479999948292972e-06,\n",
       " 'h_maloc_y': 0.00017023999989032744,\n",
       " 'kernel': 1.3183999806642532e-05,\n",
       " 'd2h': 0.00013027200102806093,\n",
       " 'mem_operations_total': 0.2673085109286476,\n",
       " 'total': 0.26732169492845426}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, s, vt = random_svd((10000, 10))\n",
    "k = len(s)\n",
    "\n",
    "reco_func = make_reconstructor(kernel_globalmem_fp64, (32, 1), timeit=True)\n",
    "reco_func(u, s, vt, k)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-d94spYeM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
