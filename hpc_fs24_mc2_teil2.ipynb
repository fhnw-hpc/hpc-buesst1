{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Ãœberblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System info:\n",
      "/home/buesst1/.cache/pypoetry/virtualenvs/hpc-mc2-buesst1-bcMvFOTC-py3.11/lib/python3.11/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "__Time Stamp__\n",
      "Report started (local time)                   : 2024-11-18 20:32:45.172829\n",
      "UTC start time                                : 2024-11-18 19:32:45.172831\n",
      "Running time (s)                              : 0.566623\n",
      "\n",
      "__Hardware Information__\n",
      "Machine                                       : x86_64\n",
      "CPU Name                                      : goldmont\n",
      "CPU Count                                     : 32\n",
      "Number of accessible CPUs                     : 32\n",
      "List of accessible CPUs cores                 : 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      "CFS Restrictions (CPUs worth of runtime)      : None\n",
      "\n",
      "CPU Features                                  : 64bit adx aes avx avx2 bmi bmi2\n",
      "                                                clflushopt clwb cmov crc32 cx16\n",
      "                                                cx8 f16c fma fsgsbase fxsr gfni\n",
      "                                                invpcid lzcnt mmx movbe pclmul\n",
      "                                                popcnt prfchw rdpid rdrnd rdseed\n",
      "                                                sahf sha sse sse2 sse3 sse4.1\n",
      "                                                sse4.2 ssse3 vaes vpclmulqdq xsave\n",
      "                                                xsavec xsaveopt xsaves\n",
      "\n",
      "Memory Total (MB)                             : 96557\n",
      "Memory Available (MB)                         : 93991\n",
      "\n",
      "__OS Information__\n",
      "Platform Name                                 : Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Platform Release                              : 5.15.146.1-microsoft-standard-WSL2\n",
      "OS Name                                       : Linux\n",
      "OS Version                                    : #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "OS Specific Version                           : ?\n",
      "Libc Version                                  : glibc 2.35\n",
      "\n",
      "__Python Information__\n",
      "Python Compiler                               : GCC 11.4.0\n",
      "Python Implementation                         : CPython\n",
      "Python Version                                : 3.11.9\n",
      "Python Locale                                 : en_US.UTF-8\n",
      "\n",
      "__Numba Toolchain Versions__\n",
      "Numba Version                                 : 0.60.0\n",
      "llvmlite Version                              : 0.43.0\n",
      "\n",
      "__LLVM Information__\n",
      "LLVM Version                                  : 14.0.6\n",
      "\n",
      "__CUDA Information__\n",
      "CUDA Device Initialized                       : True\n",
      "CUDA Driver Version                           : 12.4\n",
      "CUDA Runtime Version                          : 11.5\n",
      "CUDA NVIDIA Bindings Available                : False\n",
      "CUDA NVIDIA Bindings In Use                   : False\n",
      "CUDA Minor Version Compatibility Available    : False\n",
      "CUDA Minor Version Compatibility Needed       : False\n",
      "CUDA Minor Version Compatibility In Use       : False\n",
      "CUDA Detect Output:\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 4090'                              [SUPPORTED]\n",
      "                      Compute Capability: 8.9\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-7c34b908-8185-6853-7d22-ec916e7af144\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 64\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA Libraries Test Output:\n",
      "Finding driver from candidates:\n",
      "\t/usr/lib/wsl/lib/libcuda.so.1\n",
      "Using loader <class 'ctypes.CDLL'>\n",
      "\tTrying to load driver...\tok\n",
      "\t\tLoaded from /usr/lib/wsl/lib/libcuda.so.1\n",
      "\tMapped libcuda.so paths:\n",
      "\t\t/usr/lib/wsl/drivers/nvmdi.inf_amd64_5709e141414310f9/libcuda.so.1.1\n",
      "\t\t/usr/lib/wsl/lib/libcuda.so.1\n",
      "Finding nvvm from <unknown>\n",
      "\tLocated at libnvvm.so\n",
      "\tTrying to open library...\tok\n",
      "Finding nvrtc from <unknown>\n",
      "\tLocated at libnvrtc.so\n",
      "\tTrying to open library...\tok\n",
      "Finding cudart from <unknown>\n",
      "\tLocated at libcudart.so\n",
      "\tTrying to open library...\tok\n",
      "Finding cudadevrt from <unknown>\n",
      "\tLocated at libcudadevrt.a\n",
      "\tChecking library...\tERROR: failed to find cudadevrt:\n",
      "libcudadevrt.a not found\n",
      "Finding libdevice from Debian package\n",
      "\tLocated at /usr/lib/nvidia-cuda-toolkit/libdevice/libdevice.10.bc\n",
      "\tChecking library...\tok\n",
      "\n",
      "\n",
      "__NumPy Information__\n",
      "NumPy Version                                 : 2.0.2\n",
      "NumPy Supported SIMD features                 : ('MMX', 'SSE', 'SSE2', 'SSE3', 'SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2')\n",
      "NumPy Supported SIMD dispatch                 : ('SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2', 'AVX512F', 'AVX512CD', 'AVX512_KNL', 'AVX512_KNM', 'AVX512_SKX', 'AVX512_CLX', 'AVX512_CNL', 'AVX512_ICL')\n",
      "NumPy Supported SIMD baseline                 : ('SSE', 'SSE2', 'SSE3')\n",
      "NumPy AVX512_SKX support detected             : False\n",
      "\n",
      "__SVML Information__\n",
      "SVML State, config.USING_SVML                 : False\n",
      "SVML Library Loaded                           : False\n",
      "llvmlite Using SVML Patched LLVM              : True\n",
      "SVML Operational                              : False\n",
      "\n",
      "__Threading Layer Information__\n",
      "TBB Threading Layer Available                 : False\n",
      "+--> Disabled due to Unknown import problem.\n",
      "OpenMP Threading Layer Available              : True\n",
      "+-->Vendor: GNU\n",
      "Workqueue Threading Layer Available           : True\n",
      "+-->Workqueue imported successfully.\n",
      "\n",
      "__Numba Environment Variable Information__\n",
      "NUMBA_CUDA_DRIVER                             : /usr/lib/wsl/lib/libcuda.so.1\n",
      "\n",
      "__Conda Information__\n",
      "Conda not available.\n",
      "\n",
      "__Installed Packages__\n",
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         2.4.1\n",
      "comm              0.2.2\n",
      "contourpy         1.3.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.7\n",
      "decorator         5.1.1\n",
      "executing         2.1.0\n",
      "fonttools         4.54.1\n",
      "imageio           2.36.0\n",
      "ipykernel         6.29.5\n",
      "ipython           8.29.0\n",
      "jedi              0.19.1\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.7.2\n",
      "kiwisolver        1.4.7\n",
      "lazy_loader       0.4\n",
      "llvmlite          0.43.0\n",
      "matplotlib        3.9.2\n",
      "matplotlib-inline 0.1.7\n",
      "nest-asyncio      1.6.0\n",
      "networkx          3.4.2\n",
      "numba             0.60.0\n",
      "numpy             2.0.2\n",
      "packaging         24.1\n",
      "pandas            2.2.3\n",
      "parso             0.8.4\n",
      "pexpect           4.9.0\n",
      "pillow            11.0.0\n",
      "pip               24.0\n",
      "platformdirs      4.3.6\n",
      "prompt_toolkit    3.0.48\n",
      "psutil            6.1.0\n",
      "ptyprocess        0.7.0\n",
      "pure_eval         0.2.3\n",
      "Pygments          2.18.0\n",
      "pyparsing         3.2.0\n",
      "python-dateutil   2.9.0.post0\n",
      "pytz              2024.2\n",
      "pyzmq             26.2.0\n",
      "scikit-image      0.24.0\n",
      "scipy             1.14.1\n",
      "seaborn           0.13.2\n",
      "setuptools        69.5.1\n",
      "six               1.16.0\n",
      "stack-data        0.6.3\n",
      "tifffile          2024.9.20\n",
      "tornado           6.4.1\n",
      "traitlets         5.14.3\n",
      "typing_extensions 4.12.2\n",
      "tzdata            2024.2\n",
      "wcwidth           0.2.13\n",
      "\n",
      "No errors reported.\n",
      "\n",
      "\n",
      "__Warning log__\n",
      "Warning: Conda not available.\n",
      " Error was [Errno 2] No such file or directory: 'conda'\n",
      "\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_quota_us\n",
      "Warning (no file): /sys/fs/cgroup/cpuacct/cpu.cfs_period_us\n",
      "--------------------------------------------------------------------------------\n",
      "If requested, please copy and paste the information between\n",
      "the dashed (----) lines, or from a given specific section as\n",
      "appropriate.\n",
      "\n",
      "=============================================================\n",
      "IMPORTANT: Please ensure that you are happy with sharing the\n",
      "contents of the information present, any information that you\n",
      "wish to keep private you should remove before sharing.\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buesst1/.cache/pypoetry/virtualenvs/hpc-mc2-buesst1-bcMvFOTC-py3.11/lib/python3.11/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 6 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. WÃ¤hle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafÃ¼r Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten mÃ¶chtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_offset = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + (self._t_end - self._t_start)\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_offset = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + 1e-3*cuda.event_elapsed_time(self._t_start, self._t_end)\n",
    "\n",
    "@cuda.jit(\"void(Array(float32, 2, 'C'), Array(float32, 1, 'C'), Array(float32, 2, 'F'), int32, Array(float32, 2, 'C'))\")\n",
    "def svd_reco_kernel(u, s, vt, k, y):\n",
    "    \"\"\"SVD reconstruction for k components using cuda\n",
    "    \n",
    "    Inputs:\n",
    "    u (m,n): array\n",
    "    s (n): array (diagonal matrix)\n",
    "    vt (n,n): array\n",
    "    k int: number of reconstructed singular components\n",
    "    y (m,n): output array\n",
    "    \"\"\"\n",
    "    m, n = cuda.grid(2)\n",
    "\n",
    "    if m >= u.shape[0] or n >= vt.shape[0]:\n",
    "        return\n",
    "\n",
    "    element = 0.0\n",
    "    for p in range(k):\n",
    "        element += u[m, p] * s[p] * vt[p, n]\n",
    "\n",
    "    y[m, n] = element\n",
    "\n",
    "def calculate(u: np.ndarray, s: np.ndarray, vt: np.ndarray, k: int, block_size:tuple=(32,32)) -> np.ndarray:\n",
    "    \"\"\"Host function to perform SVD reconstruction using CUDA kernel.\n",
    "\n",
    "    Args:\n",
    "        u (np.ndarray): Left singular vectors, shape (m, n).\n",
    "        s (np.ndarray): Singular values, shape (n,).\n",
    "        vt (np.ndarray): Right singular vectors, shape (n, n).\n",
    "        k (int): Number of singular components to use in reconstruction.\n",
    "        block_size (tuple(int,int)): Number of threads per block\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reconstructed matrix, shape (m, n).\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert to correct dtype\n",
    "    u = u.astype(np.float32)\n",
    "    s = s.astype(np.float32)\n",
    "    vt = np.asfortranarray(vt.astype(np.float32))\n",
    "\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        # Ensure inputs are in the correct dtype and order\n",
    "        u = cuda.to_device(u)\n",
    "        s = cuda.to_device(s)\n",
    "        vt = cuda.to_device(vt)\n",
    "        \n",
    "        # create array where results are stored. Also pin that array so no data gets moved out of the ram.\n",
    "        m, n = u.shape[0], vt.shape[1]\n",
    "        y = cuda.device_array((m, n), dtype=np.float32)\n",
    "        y_ret = cuda.pinned_array_like(y)\n",
    "\n",
    "    # Define CUDA thread and block dimensions\n",
    "    blocks_per_grid_x = (m + block_size[0] - 1) // block_size[0]\n",
    "    blocks_per_grid_y = (n + block_size[1] - 1) // block_size[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        # Launch the CUDA kernel\n",
    "        svd_reco_kernel[blocks_per_grid, block_size](u, s, vt, k, y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        # copy back to host\n",
    "        y.copy_to_host(y_ret)\n",
    "\n",
    "    # calculate number of transfers done in each thread\n",
    "    num_transfers_per_thread = 1 + (4*k) + 1\n",
    "    number_of_GB_transferred = 1e-9*4*num_transfers_per_thread*y.shape[0]*y.shape[1]\n",
    "\n",
    "    print(f\"Cuda transfer overhead: {t_xfer.elapsed_time()*1000}ms\")\n",
    "    print(f\"Cuda kernel time: {t_kernel.elapsed_time()*1000}ms\")\n",
    "    print(f\"Consumed memory bandwidth: {number_of_GB_transferred / t_kernel.elapsed_time()} GB/s\")\n",
    "\n",
    "    return y_ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda transfer overhead: 1.002495974302292ms\n",
      "Cuda kernel time: 0.5836799740791321ms\n",
      "Consumed memory bandwidth: 119.89474216655664 GB/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04974082,  0.00398783,  0.06063343, ...,  0.06974227,\n",
       "         0.06019334, -0.04235162],\n",
       "       [ 0.02688764,  0.00190527,  0.02418954, ...,  0.03627283,\n",
       "         0.01780313, -0.00637781],\n",
       "       [ 0.0290446 , -0.02647049, -0.03107073, ...,  0.04624368,\n",
       "         0.00306286,  0.03151328],\n",
       "       ...,\n",
       "       [ 1.3522642 , -0.00273046, -0.623177  , ..., -0.6813155 ,\n",
       "        -0.4641527 , -0.96072966],\n",
       "       [ 0.2960358 ,  0.5249512 ,  0.0184323 , ..., -0.19332825,\n",
       "         0.5000806 ,  1.5635303 ],\n",
       "       [ 0.9867547 , -0.9557681 ,  1.4895947 , ...,  1.0953    ,\n",
       "        -0.5926281 ,  1.8346974 ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "subfolder = '001'\n",
    "folders = os.path.join('adni_png', subfolder)\n",
    "\n",
    "# Get all PNGs from 001 with 145 in the name\n",
    "files = sorted(glob.glob(f\"{folders}/*145.png\"))\n",
    "\n",
    "# Load all images using ImageIO and create a numpy array from them\n",
    "images = np.array([imageio.imread(f) for f in files])\n",
    "\n",
    "# Get all the names of the files\n",
    "names = [f[-17:-4] for f in files]\n",
    "\n",
    "im = images[0]\n",
    "im = im - im.min() / im.max() - im.min()  # normalize image\n",
    "u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "# convert to correct dtype\n",
    "u = u.astype(np.float32)\n",
    "s = s.astype(np.float32)\n",
    "vt = vt.astype(np.float32)\n",
    "\n",
    "calculate(u, s, vt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.48357570e-04,  6.87303161e-03,  5.32832742e-03, ...,\n",
       "         8.10194574e-03,  5.62756276e-03,  1.05523104e-02],\n",
       "       [ 5.10228332e-04, -5.43693895e-04,  8.77237471e-05, ...,\n",
       "         2.47143582e-03, -7.51565312e-05,  2.96106702e-03],\n",
       "       [ 3.63633828e-03,  3.92481964e-03,  1.46562816e-03, ...,\n",
       "         7.42339715e-03,  2.51593534e-03,  1.07109863e-02],\n",
       "       ...,\n",
       "       [ 1.30805051e+00,  1.37908542e+00,  1.05979729e+00, ...,\n",
       "         6.39316261e-01,  1.12439133e-01,  8.58197808e-02],\n",
       "       [ 1.23633599e+00,  1.39993715e+00,  1.02188718e+00, ...,\n",
       "         6.65664971e-01,  9.83572006e-02,  2.19427586e-01],\n",
       "       [ 1.16466701e+00,  1.40309119e+00,  9.09437180e-01, ...,\n",
       "         3.75059962e-01,  1.99167326e-01,  4.85913455e-02]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reconstruct_svd_for_loops3(u,s,vt,k):\n",
    "    \"\"\"SVD reconstruction for k components using 3 for-loops\n",
    "    \n",
    "    Inputs:\n",
    "    u: (m,n) numpy array\n",
    "    s: (n) numpy array (diagonal matrix)\n",
    "    vt: (n,n) numpy array\n",
    "    k: number of reconstructed singular components\n",
    "    \n",
    "    Ouput:\n",
    "    (m,n) numpy array U_mk * S_k * V^T_nk for k reconstructed components\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "\n",
    "    if k is None:\n",
    "        k = min(u.shape[0], vt.shape[0])\n",
    "\n",
    "    reco = np.zeros((u.shape[0], vt.shape[0]))\n",
    "\n",
    "    for m in range(u.shape[0]):\n",
    "        for n in range(vt.shape[0]):\n",
    "            element = 0\n",
    "            for p in range(k):\n",
    "                element += u[m, p]*s[p]*vt[p, n]\n",
    "\n",
    "            reco[m, n] = element\n",
    "            \n",
    "    ### END SOLUTION\n",
    "\n",
    "    return reco\n",
    "\n",
    "reconstruct_svd_for_loops3(u, s, vt, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-GrÃ¶sse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "FÃ¼hre 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und GrÃ¶sse der Input-Daten durch. Erstelle dafÃ¼r ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen fÃ¼r alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche BlockgrÃ¶sse in AbhÃ¤ngigkeit mit der Input-GrÃ¶sse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die GrÃ¼nde dafÃ¼r? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. FÃ¼hre wieder mehrere Experimente mit unterschiedlicher DatengrÃ¶sse durch und evaluiere den Speedup gegenÃ¼ber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grÃ¶sser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) fÃ¼r das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. DafÃ¼r kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschÃ¤rft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte fÃ¼r deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch fÃ¼r grÃ¶ssere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup fÃ¼r deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge fÃ¼r die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. ErlÃ¤utere das Komponentendigramm in 3-4 SÃ¤tzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 SÃ¤tzen begrÃ¼ndest und anhand von Beispielen erklÃ¤rst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsÃ¤chlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie kÃ¶nnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich Ã¼berrascht? Was hÃ¤ttest du zusÃ¤tzlich lernen wollen? WÃ¼rdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-bcMvFOTC-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
