{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
    "## Teil 2: GPU\n",
    "#### FHNW - FS2024\n",
    "\n",
    "Original von S. Suter, angepasst von S. Marcin und M. Stutz\n",
    "\n",
    "Abgabe von: <font color='blue'>Name hier eintragen</font>\n",
    "\n",
    "#### Ressourcen\n",
    "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
    "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
    "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
    "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe Ordner \"resources\"\n",
    "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
    "    - JIT Numba GPU 1 + 2\n",
    "    - https://youtu.be/E4REVbCVxNQ\n",
    "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
    "    - Siehe auch aktuelles Tutorial von 2021\n",
    "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIS ISSUE: https://github.com/numba/numba/issues/7104\n",
    "\n",
    "NUMBA_CUDA_DRIVER=\"/usr/lib/wsl/lib/libcuda.so.1\" python -c \"from numba import cuda; cuda.detect()\" -> this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"numba\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 6 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
       "       63.992188 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Beispiel zum testen mit Numba\n",
    "\n",
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32)'], target='cuda')\n",
    "def gpu_sqrt(x):\n",
    "    return math.sqrt(x)\n",
    "  \n",
    "a = np.arange(4096,dtype=np.float32)\n",
    "gpu_sqrt(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 GPU Rekonstruktion\n",
    "\n",
    "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest.\n",
    "\n",
    "Links:\n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "import time\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_offset = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + (self._t_end - self._t_start)\n",
    "\n",
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_offset = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exec_type, exec_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_offset + 1e-3*cuda.event_elapsed_time(self._t_start, self._t_end)\n",
    "\n",
    "def reconstruct_svd_broadcast(u,s,vt,k):\n",
    "    \"\"\"SVD reconstruction for k components using broadcast\n",
    "    \n",
    "    Inputs:\n",
    "    u: (m,n) numpy array\n",
    "    s: (n) numpy array (diagonal matrix)\n",
    "    vt: (n,n) numpy array\n",
    "    k: number of reconstructed singular components\n",
    "    \n",
    "    Ouput:\n",
    "    (m,n) numpy array U_mk * S_k * V^T_nk for k reconstructed components\n",
    "    \"\"\"\n",
    "\n",
    "    return u[:, :k]@(s[:k].reshape(-1, 1)*vt[:k, :])\n",
    "\n",
    "@cuda.jit(\"void(Array(float64, 2, 'C'), Array(float64, 1, 'C'), Array(float64, 2, 'F'), int32, Array(float64, 2, 'C'))\")\n",
    "def svd_reco_kernel(u, s, vt, k, y):\n",
    "    \"\"\"SVD reconstruction for k components using cuda\n",
    "    \n",
    "    Inputs:\n",
    "    u (m,n): array\n",
    "    s (n): array (diagonal matrix)\n",
    "    vt (n,n): array\n",
    "    k int: number of reconstructed singular components\n",
    "    y (m,n): output array\n",
    "    \"\"\"\n",
    "    m, n = cuda.grid(2)\n",
    "\n",
    "    if m >= u.shape[0] or n >= vt.shape[1]:\n",
    "        return\n",
    "\n",
    "    element = 0.0\n",
    "    for p in range(k):\n",
    "        element += u[m, p] * s[p] * vt[p, n]\n",
    "\n",
    "    y[m, n] = element\n",
    "\n",
    "def get_transfers_fpo_per_thread(k):\n",
    "    \"\"\" Standard function to estimate number of data transfers and operations per thread\n",
    "    \"\"\"\n",
    "    \n",
    "    num_transfers_per_thread = 1 + (4 * k) + 1\n",
    "    num_fpo_per_thread = k*2\n",
    "\n",
    "    return num_transfers_per_thread, num_fpo_per_thread\n",
    "\n",
    "def svd_reco_cuda(\n",
    "    u: np.ndarray, s: np.ndarray, vt: np.ndarray, k: np.int32, block_size: tuple = (32, 32)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Host function to perform SVD reconstruction using CUDA kernel.\n",
    "\n",
    "    Args:\n",
    "        u (np.ndarray): Left singular vectors, shape (m, n).\n",
    "        s (np.ndarray): Singular values, shape (n,).\n",
    "        vt (np.ndarray): Right singular vectors, shape (n, n).\n",
    "        k (int32): Number of singular components to use in reconstruction.\n",
    "        block_size (tuple(int,int)): Number of threads per block\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reconstructed matrix, shape (m, n).\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to correct dtype\n",
    "    u = u.astype(np.float64)\n",
    "    s = s.astype(np.float64)\n",
    "    vt = np.asfortranarray(vt.astype(np.float64)) # convert and make column major\n",
    "\n",
    "    # Send arrays to gpu\n",
    "    u = cuda.to_device(u)\n",
    "    s = cuda.to_device(s)\n",
    "    vt = cuda.to_device(vt)\n",
    "\n",
    "    # create array where results are stored. Also pin that array so no data gets moved out of the ram.\n",
    "    m, n = u.shape[0], vt.shape[1]\n",
    "    y = cuda.device_array((m, n), dtype=np.float64)\n",
    "    y_ret = cuda.pinned_array_like(y)\n",
    "\n",
    "    # Define CUDA thread and block dimensions\n",
    "    blocks_per_grid_x = (m + block_size[0] - 1) // block_size[0]\n",
    "    blocks_per_grid_y = (n + block_size[1] - 1) // block_size[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    # launch cuda kernel\n",
    "    svd_reco_kernel[blocks_per_grid, block_size](u, s, vt, np.int32(k), y)\n",
    "\n",
    "    y.copy_to_host(y_ret)\n",
    "\n",
    "    return y_ret\n",
    "\n",
    "def svd_reco_cuda_perfmeasure(u: np.ndarray, s: np.ndarray, vt: np.ndarray, k: np.int32, block_size: tuple):\n",
    "    \"\"\"Host function to perform SVD reconstruction using CUDA kernel and doing performance measurements.\n",
    "\n",
    "    Args:\n",
    "        u (np.ndarray): Left singular vectors, shape (m, n).\n",
    "        s (np.ndarray): Singular values, shape (n,).\n",
    "        vt (np.ndarray): Right singular vectors, shape (n, n).\n",
    "        k (int32): Number of singular components to use in reconstruction.\n",
    "        block_size (tuple(int,int)): Number of threads per block\n",
    "\n",
    "    Returns:\n",
    "        dict containing times including memory bandwidth and TFLOPs\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to correct dtype\n",
    "    u = u.astype(np.float64)\n",
    "    s = s.astype(np.float64)\n",
    "    vt = vt.astype(np.float64)\n",
    "\n",
    "    # reconstruct using reference function\n",
    "    y_ref = reconstruct_svd_broadcast(u, s, vt, k)\n",
    "\n",
    "    # make column major\n",
    "    vt = np.asfortranarray(vt)\n",
    "\n",
    "    # start profiling\n",
    "    cuda.profile_start()\n",
    "\n",
    "    with time_region_cuda() as t_xfer:\n",
    "        # Send arrays to gpu\n",
    "        u = cuda.to_device(u)\n",
    "        s = cuda.to_device(s)\n",
    "        vt = cuda.to_device(vt)\n",
    "\n",
    "        # create array where results are stored. Also pin that array so no data gets moved out of the ram.\n",
    "        m, n = u.shape[0], vt.shape[1]\n",
    "        y = cuda.device_array((m, n), dtype=np.float64)\n",
    "        y_ret = cuda.pinned_array_like(y)\n",
    "\n",
    "    # Define CUDA thread and block dimensions\n",
    "    blocks_per_grid_x = (m + block_size[0] - 1) // block_size[0]\n",
    "    blocks_per_grid_y = (n + block_size[1] - 1) // block_size[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "    with time_region_cuda() as t_kernel:\n",
    "        # Launch the CUDA kernel\n",
    "        svd_reco_kernel[blocks_per_grid, block_size](u, s, vt, np.int32(k), y)\n",
    "\n",
    "    with time_region_cuda(t_xfer.elapsed_time()) as t_xfer:\n",
    "        # copy back to host\n",
    "        y.copy_to_host(y_ret)\n",
    "\n",
    "    # stop profiling\n",
    "    cuda.profile_stop()\n",
    "    \n",
    "    # get number of transfers and number of fp64 operations per thread\n",
    "    num_transfers_per_thread, num_fpo_per_thread = get_transfers_fpo_per_thread(k)\n",
    "\n",
    "    # calculate number of transfers in total\n",
    "    number_of_GB_transferred_total = (\n",
    "        1e-9 * 8 * num_transfers_per_thread * y.shape[0] * y.shape[1]\n",
    "    )\n",
    "\n",
    "    # calculate number of floating point operations\n",
    "    num_fpo_total = 1e-12 * num_fpo_per_thread * y.shape[0] * y.shape[1]\n",
    "    \n",
    "    return {\n",
    "        \"time_transfer_ms\": t_xfer.elapsed_time()*1000,\n",
    "        \"time_kernel_ms\": t_kernel.elapsed_time()*1000,\n",
    "        \"consumed_mem_bandwidth_GB/s\": number_of_GB_transferred_total / t_kernel.elapsed_time(),\n",
    "        \"consumed TFLOPs\": num_fpo_total / t_kernel.elapsed_time(),\n",
    "        \"isclose\": np.isclose(y_ref, y_ret).all()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hpc-mc2-buesst1-d94spYeM-py3.11\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 48 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time_transfer_ms': 4.386815756559372,\n",
       " 'time_kernel_ms': 0.9410560131072999,\n",
       " 'consumed_mem_bandwidth_GB/s': 252.31773315593952,\n",
       " 'consumed TFLOPs': 0.015723612403412652,\n",
       " 'isclose': array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import imageio.v3 as imageio\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "subfolder = '001'\n",
    "folders = os.path.join('adni_png', subfolder)\n",
    "\n",
    "# Get all PNGs from 001 with 145 in the name\n",
    "files = sorted(glob.glob(f\"{folders}/*145.png\"))\n",
    "\n",
    "# Load all images using ImageIO and create a numpy array from them\n",
    "images = np.array([imageio.imread(f) for f in files])\n",
    "\n",
    "# Get all the names of the files\n",
    "names = [f[-17:-4] for f in files]\n",
    "\n",
    "im = images[0]\n",
    "im = im - im.min() / im.max() - im.min()  # normalize image\n",
    "u, s, vt = np.linalg.svd(im, full_matrices=False)\n",
    "\n",
    "svd_reco_cuda_perfmeasure(u, s, vt, len(s), (32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 GPU-Kernel Performance\n",
    "\n",
    "##### 5.3.1 Blocks und Input-Grösse\n",
    "\n",
    "Links: \n",
    "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
    "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
    "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
    "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
    "\n",
    "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Grösse der Input-Daten durch. Erstelle dafür ein neues Datenset mit beliebig grossen Matrizen, da die GPU besonders geeignet ist um grosse Inputs zu verarbeiten (Verwende diese untschiedlich grossen Matrizen für alle nachfolgenden Vergeliche und Tasks ebenfalls). Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse in Abhängigkeit mit der Input-Grösse hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und GPU Implementierung? Diskutiere deine Analyse (ggf. mit Grafiken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Shared Memory auf der GPU\n",
    "Optimiere deine Implementierung von oben indem du das shared Memory der GPU verwendest. Führe wieder mehrere Experimente mit unterschiedlicher Datengrösse durch und evaluiere den Speedup gegenüber der CPU Implementierung.\n",
    "\n",
    "Links:\n",
    "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
    "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf die GPU? Interpretiere deine Resultate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Bonus: Weitere Optimierungen\n",
    "Optimiere deine Implementation von oben weiter. Damit du Erfolg hast, muss der Data-Reuse noch grösser sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 NVIDIA Profiler\n",
    "\n",
    "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
    "\n",
    "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
    "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
    "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
    "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
    "\n",
    "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
    "\n",
    "\n",
    "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe, welche Bottlenecks du gefunden bzw. entschärft hast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
    "#### 6.1 Implementierung\n",
    "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Untersuche dies auch für grössere Inputs als die MRI-Bilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.2 Analyse\n",
    "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.3 Komponentendiagramm\n",
    "\n",
    "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7 Reflexion\n",
    "\n",
    "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "6: Reflektiere die Mini-Challenge. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
    "\n",
    "<font color='blue'>Antwort hier eingeben</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc-mc2-buesst1-d94spYeM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
